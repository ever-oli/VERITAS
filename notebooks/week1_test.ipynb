{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/VERITAS\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 6 (delta 2), reused 6 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (6/6), 3.45 KiB | 3.45 MiB/s, done.\n",
      "From https://github.com/ever-oli/VERITAS\n",
      "   81345e0..8195f3e  main       -> origin/main\n",
      "Updating 81345e0..8195f3e\n",
      "Fast-forward\n",
      " models/audio_to_ppr.py     | 101 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m---\u001b[m\n",
      " notebooks/week1_test.ipynb | 172 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
      " 2 files changed, 265 insertions(+), 8 deletions(-)\n",
      " create mode 100644 notebooks/week1_test.ipynb\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 6 (delta 2), reused 6 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (6/6), 3.45 KiB | 3.45 MiB/s, done.\n",
      "From https://github.com/ever-oli/VERITAS\n",
      "   81345e0..8195f3e  main       -> origin/main\n",
      "Updating 81345e0..8195f3e\n",
      "Fast-forward\n",
      " models/audio_to_ppr.py     | 101 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m---\u001b[m\n",
      " notebooks/week1_test.ipynb | 172 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
      " 2 files changed, 265 insertions(+), 8 deletions(-)\n",
      " create mode 100644 notebooks/week1_test.ipynb\n",
      "/content\n",
      "Installing dependencies (manual order to avoid TF resolver)...\n",
      "/content\n",
      "Installing dependencies (manual order to avoid TF resolver)...\n",
      "✓ Setup complete. Repo path: /content/VERITAS\n",
      "✓ Setup complete. Repo path: /content/VERITAS\n"
     ]
    }
   ],
   "source": [
    "# --- COLAB SETUP CELL ---\n",
    "import os\n",
    "import sys\n",
    "\n",
    "USER = \"ever-oli\"\n",
    "REPO = \"VERITAS\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/{USER}/{REPO}.git\n",
    "else:\n",
    "    %cd {REPO}\n",
    "    !git pull\n",
    "    %cd ..\n",
    "\n",
    "repo_path = os.path.abspath(REPO)\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "print(\"Installing dependencies (PyTorch-based, no TensorFlow)...\")\n",
    "\n",
    "# Core audio/ML deps\n",
    "!pip install -q librosa scipy matplotlib seaborn tqdm soundfile transformers pretty_midi\n",
    "\n",
    "# PyTorch (already in Colab usually)\n",
    "!pip install -q torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Demucs\n",
    "!pip install -q demucs\n",
    "\n",
    "# Basic Pitch PyTorch (no TF!)\n",
    "!pip install -q git+https://github.com/gudgud96/basic-pitch-torch.git\n",
    "\n",
    "print(f\"✓ Setup complete. Repo path: {repo_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_audio = os.path.join(\"VERITAS\", \"test_jazz.mp3\")\n",
    "!wget -O {test_audio} https://files.freemusicarchive.org/storage-freemusicarchive-org/music/WFMU/Broke_For_Free/Directionless_EP/Broke_For_Free_-_01_-_Night_Owl.mp3\n",
    "print(f\"Saved to: {os.path.abspath(test_audio)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd33b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnxruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mort\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class NeuralNoteInference:\n",
    "    def __init__(self, model_path=\"path/to/features_model.onnx\"):\n",
    "        self.session = ort.InferenceSession(model_path)\n",
    "        # TODO: load CNN weights if needed\n",
    "    \n",
    "    def extract_ppr(self, audio_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "        Returns:\n",
    "            (Time, 88) numpy array of note probabilities\n",
    "        \"\"\"\n",
    "        # TODO: load audio, preprocess, run inference, postprocess\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbeae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioToPPR initialized on cuda\n",
      "Step 1: Running Demucs...\n",
      "Error: 'AudioToPPR' object has no attribute 'separate_stems'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure repo path is on sys.path (in case cell 1 wasn't re-run)\n",
    "repo_path = os.path.abspath(\"VERITAS\")\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "from models.audio_to_midi_image import AudioToMidiImage\n",
    "\n",
    "# Path to the downloaded test audio\n",
    "test_audio = os.path.join(\"VERITAS\", \"test_jazz.mp3\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = AudioToMidiImage()\n",
    "\n",
    "try:\n",
    "    # 1. Separate ALL stems (vocals, drums, bass, other)\n",
    "    print(\"Step 1: Running Demucs stem separation...\")\n",
    "    stems = pipeline.separate_stems(test_audio)\n",
    "    print(f\"Generated stems: {list(stems.keys())}\")\n",
    "    \n",
    "    # 2. Generate MIDI and images for each stem\n",
    "    print(\"\\nStep 2: Converting each stem to MIDI + piano roll images...\")\n",
    "    results = pipeline.stems_to_midi(stems)\n",
    "    \n",
    "    print(\"\\n✅ Success! Multi-stem MIDI + image generation complete.\")\n",
    "    print(f\"Results saved to: {os.path.dirname(list(results.values())[0]['midi'])}\")\n",
    "    \n",
    "    # Show what was generated\n",
    "    for stem_name, paths in results.items():\n",
    "        print(f\"- {stem_name}: MIDI + Image generated\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4a143",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Why This Approach:**\n",
    "- Uses NeuralNote's optimized models directly (no TF hell)\n",
    "- Gets the visual output you want for VLM\n",
    "- Simple, plug-and-play with existing pipeline\n",
    "\n",
    "**If NeuralNote's models are too C++-heavy**, suggest the next best: `piano-transcription-inference` (pure PyTorch, outputs MIDI, easy to render).\n",
    "\n",
    "Focus on **getting a working pipeline end-to-end**. I can refine once I have basic transcription + visualization.\n",
    "\n",
    "---\n",
    "\n",
    "**Why This Prompt is Better:**\n",
    "1. **Focuses on NeuralNote's models** - directly from `Lib/ModelData/`, as you specified\n",
    "2. **Simple output** - MIDI → image, no matrices\n",
    "3. **Practical** - Colab cells that work\n",
    "4. **Aligns with your goal** - piano roll image for VLM/MusicLM\n",
    "\n",
    "Send this to Gemini. They should come back with code that loads NeuralNote's models and renders the piano roll image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
